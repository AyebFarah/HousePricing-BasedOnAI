{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46292b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PARTIE 1: FONDEMENTS THÃ‰ORIQUES DES MODÃˆLES\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "        ğŸŒ³ ARBRE DE DÃ‰CISION (Decision Tree)\n",
      "\n",
      "        Principe:\n",
      "        - Divise rÃ©cursivement l'espace des features en rÃ©gions rectangulaires\n",
      "        - Ã€ chaque nÅ“ud, choisit la meilleure variable et seuil pour minimiser l'erreur\n",
      "        - PrÃ©diction = moyenne des valeurs dans chaque feuille\n",
      "\n",
      "        Avantages:\n",
      "        âœ“ InterprÃ©table et visualisable\n",
      "        âœ“ Capture les relations non-linÃ©aires\n",
      "        âœ“ Pas besoin de normalisation\n",
      "        âœ“ TrÃ¨s rapide\n",
      "\n",
      "        InconvÃ©nients:\n",
      "        âœ— Tendance au sur-apprentissage\n",
      "        âœ— Instable (petits changements = grands impacts)\n",
      "        âœ— Moins performant que les ensembles\n",
      "\n",
      "        HyperparamÃ¨tres clÃ©s:\n",
      "        â€¢ max_depth: Profondeur maximale (contrÃ´le la complexitÃ©)\n",
      "        â€¢ min_samples_split: Minimum d'Ã©chantillons pour diviser un nÅ“ud\n",
      "        â€¢ min_samples_leaf: Minimum d'Ã©chantillons par feuille\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        ğŸŒ² FORÃŠT ALÃ‰ATOIRE (Random Forest)\n",
      "\n",
      "        Principe:\n",
      "        - Ensemble de nombreux arbres de dÃ©cision\n",
      "        - Chaque arbre entraÃ®nÃ© sur un Ã©chantillon alÃ©atoire (bootstrap)\n",
      "        - PrÃ©diction = moyenne des prÃ©dictions de tous les arbres\n",
      "        - \"Wisdom of the crowd\" rÃ©duit la variance\n",
      "\n",
      "        Avantages:\n",
      "        âœ“ TrÃ¨s robuste au sur-apprentissage\n",
      "        âœ“ GÃ¨re bien les donnÃ©es bruitÃ©es\n",
      "        âœ“ Fournit l'importance des features\n",
      "        âœ“ ParallÃ©lisable (rapide avec multi-threading)\n",
      "\n",
      "        InconvÃ©nients:\n",
      "        âœ— Moins interprÃ©table qu'un seul arbre\n",
      "        âœ— Plus lent que les arbres simples\n",
      "        âœ— Consomme plus de mÃ©moire\n",
      "\n",
      "        HyperparamÃ¨tres clÃ©s:\n",
      "        â€¢ n_estimators: Nombre d'arbres (plus = meilleur mais plus lent)\n",
      "        â€¢ max_depth: Profondeur des arbres\n",
      "        â€¢ max_features: Nombre de features considÃ©rÃ©es par split\n",
      "        â€¢ min_samples_leaf: RÃ©gularisation\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        ğŸš€ GRADIENT BOOSTING\n",
      "\n",
      "        Principe:\n",
      "        - Construit les arbres sÃ©quentiellement\n",
      "        - Chaque nouvel arbre corrige les erreurs du prÃ©cÃ©dent\n",
      "        - Minimise une fonction de perte par descente de gradient\n",
      "        - Combine des \"weak learners\" en un \"strong learner\"\n",
      "\n",
      "        Avantages:\n",
      "        âœ“ Souvent le plus performant sur des donnÃ©es tabulaires\n",
      "        âœ“ Capture des relations complexes\n",
      "        âœ“ Moins sensible aux outliers que Random Forest\n",
      "        âœ“ FlexibilitÃ© dans la fonction de perte\n",
      "\n",
      "        InconvÃ©nients:\n",
      "        âœ— Sensible au sur-apprentissage si mal paramÃ©trÃ©\n",
      "        âœ— Plus lent (sÃ©quentiel, non parallÃ©lisable)\n",
      "        âœ— NÃ©cessite un tuning minutieux\n",
      "\n",
      "        HyperparamÃ¨tres clÃ©s:\n",
      "        â€¢ n_estimators: Nombre d'arbres\n",
      "        â€¢ learning_rate: Taux d'apprentissage (plus petit = plus robuste)\n",
      "        â€¢ max_depth: Profondeur (gÃ©nÃ©ralement plus faible que RF)\n",
      "        â€¢ subsample: Fraction d'Ã©chantillons par arbre (rÃ©gularisation)\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        âš¡ XGBoost (eXtreme Gradient Boosting)\n",
      "\n",
      "        Principe:\n",
      "        - Version optimisÃ©e et rÃ©gularisÃ©e du Gradient Boosting\n",
      "        - Utilise des techniques avancÃ©es (regularization, pruning, parallelization)\n",
      "        - Algorithme de splitting plus efficace\n",
      "        - Gestion native des valeurs manquantes\n",
      "\n",
      "        Avantages:\n",
      "        âœ“ Souvent le meilleur en compÃ©titions (Kaggle)\n",
      "        âœ“ Plus rapide que Gradient Boosting classique\n",
      "        âœ“ RÃ©gularisation L1/L2 intÃ©grÃ©e\n",
      "        âœ“ Gestion automatique des missing values\n",
      "\n",
      "        InconvÃ©nients:\n",
      "        âœ— Beaucoup d'hyperparamÃ¨tres Ã  tuner\n",
      "        âœ— Peut Ãªtre \"overkill\" pour des problÃ¨mes simples\n",
      "        âœ— NÃ©cessite comprÃ©hension approfondie\n",
      "\n",
      "        HyperparamÃ¨tres clÃ©s:\n",
      "        â€¢ n_estimators: Nombre d'arbres\n",
      "        â€¢ learning_rate: Taux d'apprentissage\n",
      "        â€¢ max_depth: Profondeur des arbres\n",
      "        â€¢ colsample_bytree: Fraction de features par arbre\n",
      "        â€¢ reg_alpha/reg_lambda: RÃ©gularisation L1/L2\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        ğŸ§  RÃ‰SEAU DE NEURONES (Multi-Layer Perceptron)\n",
      "\n",
      "        Principe:\n",
      "        - RÃ©seau de neurones artificiels organisÃ©s en couches\n",
      "        - Chaque neurone applique: activation(weighted_sum(inputs) + bias)\n",
      "        - Apprentissage par rÃ©tropropagation du gradient\n",
      "        - Peut approximer n'importe quelle fonction (thÃ©orÃ¨me d'approximation universelle)\n",
      "\n",
      "        Avantages:\n",
      "        âœ“ Peut capturer des relations trÃ¨s complexes\n",
      "        âœ“ Flexible et adaptable\n",
      "        âœ“ Bonne gÃ©nÃ©ralisation avec assez de donnÃ©es\n",
      "\n",
      "        InconvÃ©nients:\n",
      "        âœ— NÃ©cessite BEAUCOUP de donnÃ©es (milliers/millions)\n",
      "        âœ— BoÃ®te noire (difficile Ã  interprÃ©ter)\n",
      "        âœ— Sensible au scaling des features\n",
      "        âœ— Lent Ã  entraÃ®ner\n",
      "        âœ— Instable (rÃ©sultats variables)\n",
      "\n",
      "        HyperparamÃ¨tres clÃ©s:\n",
      "        â€¢ hidden_layer_sizes: Architecture (nombre et taille des couches)\n",
      "        â€¢ activation: Fonction d'activation (relu, tanh)\n",
      "        â€¢ alpha: RÃ©gularisation L2\n",
      "        â€¢ learning_rate_init: Taux d'apprentissage initial\n",
      "\n",
      "        Note: GÃ©nÃ©ralement sous-performant sur petits datasets tabulaires\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "PARTIE 2: CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES\n",
      "====================================================================================================\n",
      "\n",
      "âœ“ Dataset: 4856 propriÃ©tÃ©s, 8 colonnes\n",
      "\n",
      "ğŸ“Š Feature Engineering (sans data leakage)...\n",
      "  â†’ CrÃ©ation du luxury_score (pas de leakage)\n",
      "  â†’ Features crÃ©Ã©es: luxury_score, property_tier, is_premium_location\n",
      "  âœ“ Aucune information du prix utilisÃ©e!\n",
      "\n",
      "ğŸ“Š PrÃ©paration des donnÃ©es pour cross-validation...\n",
      "  Note: Le prix mÃ©dian par zone sera calculÃ© sur train set uniquement\n",
      "\n",
      "====================================================================================================\n",
      "PARTIE 3: OPTIMISATION DES HYPERPARAMÃˆTRES (Grid Search CV)\n",
      "====================================================================================================\n",
      "\n",
      "Cette Ã©tape peut prendre 15-30 minutes selon votre machine...\n",
      "Grid Search utilise 3-fold CV pour chaque combinaison de paramÃ¨tres.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ Optimisation: Decision Tree\n",
      "================================================================================\n",
      "Nombre de combinaisons Ã  tester: 36\n",
      "Testing 36 combinations...\n",
      "  Progress: 30/36...\n",
      "âœ“ TerminÃ© en 4.2s\n",
      "Meilleur score RÂ²: 0.9339\n",
      "Meilleurs paramÃ¨tres:\n",
      "  â€¢ max_depth: 10\n",
      "  â€¢ min_samples_leaf: 15\n",
      "  â€¢ min_samples_split: 10\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ Optimisation: Random Forest\n",
      "================================================================================\n",
      "Nombre de combinaisons Ã  tester: 162\n",
      "Testing 162 combinations...\n",
      "  Progress: 160/162...\n",
      "âœ“ TerminÃ© en 114.0s\n",
      "Meilleur score RÂ²: 0.9344\n",
      "Meilleurs paramÃ¨tres:\n",
      "  â€¢ max_depth: 15\n",
      "  â€¢ max_features: sqrt\n",
      "  â€¢ min_samples_leaf: 2\n",
      "  â€¢ min_samples_split: 5\n",
      "  â€¢ n_estimators: 300\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ Optimisation: Gradient Boosting\n",
      "================================================================================\n",
      "Nombre de combinaisons Ã  tester: 486\n",
      "Testing 486 combinations...\n",
      "  Progress: 480/486...\n",
      "âœ“ TerminÃ© en 612.9s\n",
      "Meilleur score RÂ²: 0.9377\n",
      "Meilleurs paramÃ¨tres:\n",
      "  â€¢ learning_rate: 0.05\n",
      "  â€¢ max_depth: 4\n",
      "  â€¢ max_features: sqrt\n",
      "  â€¢ min_samples_split: 10\n",
      "  â€¢ n_estimators: 200\n",
      "  â€¢ subsample: 0.8\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ Optimisation: XGBoost\n",
      "================================================================================\n",
      "Nombre de combinaisons Ã  tester: 729\n",
      "Testing 729 combinations...\n",
      "  Progress: 720/729...\n",
      "âœ“ TerminÃ© en 497.4s\n",
      "Meilleur score RÂ²: 0.9377\n",
      "Meilleurs paramÃ¨tres:\n",
      "  â€¢ colsample_bytree: 0.9\n",
      "  â€¢ learning_rate: 0.05\n",
      "  â€¢ max_depth: 4\n",
      "  â€¢ min_child_weight: 5\n",
      "  â€¢ n_estimators: 100\n",
      "  â€¢ subsample: 0.8\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ Optimisation: MLP\n",
      "================================================================================\n",
      "Nombre de combinaisons Ã  tester: 36\n",
      "Testing 36 combinations...\n",
      "  Progress: 30/36...\n",
      "âœ“ TerminÃ© en 156.6s\n",
      "Meilleur score RÂ²: 0.9344\n",
      "Meilleurs paramÃ¨tres:\n",
      "  â€¢ activation: tanh\n",
      "  â€¢ alpha: 0.01\n",
      "  â€¢ hidden_layer_sizes: (128, 64)\n",
      "  â€¢ learning_rate_init: 0.001\n",
      "\n",
      "====================================================================================================\n",
      "PARTIE 4: Ã‰VALUATION FINALE DES MODÃˆLES OPTIMISÃ‰S\n",
      "====================================================================================================\n",
      "âœ“ Features finales: 46 colonnes\n",
      "âœ“ Target: log10(price)\n",
      "\n",
      "ğŸ“Š Ã‰valuation finale: Decision Tree...\n",
      "\n",
      "ğŸ“Š Ã‰valuation finale: Random Forest...\n",
      "\n",
      "ğŸ“Š Ã‰valuation finale: Gradient Boosting...\n",
      "\n",
      "ğŸ“Š Ã‰valuation finale: XGBoost...\n",
      "\n",
      "ğŸ“Š Ã‰valuation finale: MLP...\n",
      "\n",
      "====================================================================================================\n",
      "TABLEAU RÃ‰CAPITULATIF DES PERFORMANCES FINALES\n",
      "====================================================================================================\n",
      "\n",
      "                    R2_log  R2_std  RMSE_log  MAE_log  RMSE_actual   MAE_actual     MAPE\n",
      "XGBoost            0.9383  0.0086    0.3094   0.2063  518469.4916  159388.8298  71.7874\n",
      "Gradient Boosting  0.9383  0.0083    0.3094   0.2071  518260.2530  160183.5492  71.8745\n",
      "MLP                0.9361  0.0086    0.3148   0.2114  516499.0297  161012.0844  73.7399\n",
      "Random Forest      0.9359  0.0081    0.3155   0.2131  519278.5395  161379.9077  73.6874\n",
      "Decision Tree      0.9341  0.0080    0.3197   0.2195  521863.7506  167680.9306  75.2981\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ† MODÃˆLE CHAMPION\n",
      "====================================================================================================\n",
      "\n",
      "Le meilleur modÃ¨le est: XGBoost\n",
      "  â€¢ RÂ² = 0.9383\n",
      "  â€¢ MAE = 159,389 TND\n",
      "  â€¢ MAPE = 71.79%\n",
      "\n",
      "ğŸ“ JUSTIFICATION DU CHOIX:\n",
      "  XGBoost a Ã©tÃ© sÃ©lectionnÃ© car il prÃ©sente:\n",
      "  1. Le meilleur RÂ² (0.9383) = meilleure capacitÃ© explicative\n",
      "  2. MAPE acceptable (71.8%) pour l'immobilier\n",
      "  3. Robustesse confirmÃ©e par validation croisÃ©e\n",
      "\n",
      "ğŸ’¡ NOTE SUR LE DATA LEAKAGE:\n",
      "  Ce modÃ¨le utilise des features SANS data leakage:\n",
      "  â€¢ luxury_score: calculÃ© Ã  partir de features disponibles\n",
      "  â€¢ location_price_level: calculÃ© sur train set uniquement en CV\n",
      "  â€¢ property_tier: basÃ© sur luxury_score\n",
      "  âœ“ Aucune information du prix target utilisÃ©e!\n",
      "\n",
      "ğŸ“¦ EntraÃ®nement du modÃ¨le final sur toutes les donnÃ©es...\n",
      "\n",
      "âœ… ModÃ¨le sauvegardÃ©:\n",
      "  â€¢ ../output/best_model.pkl\n",
      "  â€¢ ../output/scaler.pkl\n",
      "  â€¢ ../output/feature_cols.pkl\n",
      "  â€¢ ../output/location_stats.pkl (pour production)\n",
      "  â€¢ ../output/premium_locations.pkl\n",
      "  â€¢ ../output/final_comparison.csv\n",
      "\n",
      "====================================================================================================\n",
      "âœ… PROJET TERMINÃ‰!\n",
      "====================================================================================================\n",
      "\n",
      "ProcÃ©dez maintenant Ã  'prediction_interface.py' pour tester le modÃ¨le!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PROJET UNIVERSITAIRE: PRÃ‰DICTION DES PRIX IMMOBILIERS EN TUNISIE\n",
    "================================================================================\n",
    "Auteur: [Votre Nom]\n",
    "Date: Novembre 2024\n",
    "Cours: Machine Learning / Data Science\n",
    "\n",
    "Description:\n",
    "    Ce projet compare 5 modÃ¨les de rÃ©gression non-linÃ©aires pour prÃ©dire\n",
    "    les prix de l'immobilier en Tunisie, avec optimisation des hyperparamÃ¨tres\n",
    "    et interface de prÃ©diction.\n",
    "    \n",
    "    Note: Utilise des features sans data leakage (prix mÃ©dian par zone calculÃ©\n",
    "    sur train set uniquement lors de la cross-validation).\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# ModÃ¨les\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"PARTIE 1: FONDEMENTS THÃ‰ORIQUES DES MODÃˆLES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "models_theory = {\n",
    "    \"Decision Tree\": {\n",
    "        \"description\": \"\"\"\n",
    "        ğŸŒ³ ARBRE DE DÃ‰CISION (Decision Tree)\n",
    "        \n",
    "        Principe:\n",
    "        - Divise rÃ©cursivement l'espace des features en rÃ©gions rectangulaires\n",
    "        - Ã€ chaque nÅ“ud, choisit la meilleure variable et seuil pour minimiser l'erreur\n",
    "        - PrÃ©diction = moyenne des valeurs dans chaque feuille\n",
    "        \n",
    "        Avantages:\n",
    "        âœ“ InterprÃ©table et visualisable\n",
    "        âœ“ Capture les relations non-linÃ©aires\n",
    "        âœ“ Pas besoin de normalisation\n",
    "        âœ“ TrÃ¨s rapide\n",
    "        \n",
    "        InconvÃ©nients:\n",
    "        âœ— Tendance au sur-apprentissage\n",
    "        âœ— Instable (petits changements = grands impacts)\n",
    "        âœ— Moins performant que les ensembles\n",
    "        \n",
    "        HyperparamÃ¨tres clÃ©s:\n",
    "        â€¢ max_depth: Profondeur maximale (contrÃ´le la complexitÃ©)\n",
    "        â€¢ min_samples_split: Minimum d'Ã©chantillons pour diviser un nÅ“ud\n",
    "        â€¢ min_samples_leaf: Minimum d'Ã©chantillons par feuille\n",
    "        \"\"\",\n",
    "        \n",
    "        \"param_grid\": {\n",
    "            'max_depth': [10, 15, 20, 25],\n",
    "            'min_samples_split': [10, 20, 30],\n",
    "            'min_samples_leaf': [5, 10, 15]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Random Forest\": {\n",
    "        \"description\": \"\"\"\n",
    "        ğŸŒ² FORÃŠT ALÃ‰ATOIRE (Random Forest)\n",
    "        \n",
    "        Principe:\n",
    "        - Ensemble de nombreux arbres de dÃ©cision\n",
    "        - Chaque arbre entraÃ®nÃ© sur un Ã©chantillon alÃ©atoire (bootstrap)\n",
    "        - PrÃ©diction = moyenne des prÃ©dictions de tous les arbres\n",
    "        - \"Wisdom of the crowd\" rÃ©duit la variance\n",
    "        \n",
    "        Avantages:\n",
    "        âœ“ TrÃ¨s robuste au sur-apprentissage\n",
    "        âœ“ GÃ¨re bien les donnÃ©es bruitÃ©es\n",
    "        âœ“ Fournit l'importance des features\n",
    "        âœ“ ParallÃ©lisable (rapide avec multi-threading)\n",
    "        \n",
    "        InconvÃ©nients:\n",
    "        âœ— Moins interprÃ©table qu'un seul arbre\n",
    "        âœ— Plus lent que les arbres simples\n",
    "        âœ— Consomme plus de mÃ©moire\n",
    "        \n",
    "        HyperparamÃ¨tres clÃ©s:\n",
    "        â€¢ n_estimators: Nombre d'arbres (plus = meilleur mais plus lent)\n",
    "        â€¢ max_depth: Profondeur des arbres\n",
    "        â€¢ max_features: Nombre de features considÃ©rÃ©es par split\n",
    "        â€¢ min_samples_leaf: RÃ©gularisation\n",
    "        \"\"\",\n",
    "        \n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [15, 20, 25],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'min_samples_leaf': [2, 4, 6],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Gradient Boosting\": {\n",
    "        \"description\": \"\"\"\n",
    "        ğŸš€ GRADIENT BOOSTING\n",
    "        \n",
    "        Principe:\n",
    "        - Construit les arbres sÃ©quentiellement\n",
    "        - Chaque nouvel arbre corrige les erreurs du prÃ©cÃ©dent\n",
    "        - Minimise une fonction de perte par descente de gradient\n",
    "        - Combine des \"weak learners\" en un \"strong learner\"\n",
    "        \n",
    "        Avantages:\n",
    "        âœ“ Souvent le plus performant sur des donnÃ©es tabulaires\n",
    "        âœ“ Capture des relations complexes\n",
    "        âœ“ Moins sensible aux outliers que Random Forest\n",
    "        âœ“ FlexibilitÃ© dans la fonction de perte\n",
    "        \n",
    "        InconvÃ©nients:\n",
    "        âœ— Sensible au sur-apprentissage si mal paramÃ©trÃ©\n",
    "        âœ— Plus lent (sÃ©quentiel, non parallÃ©lisable)\n",
    "        âœ— NÃ©cessite un tuning minutieux\n",
    "        \n",
    "        HyperparamÃ¨tres clÃ©s:\n",
    "        â€¢ n_estimators: Nombre d'arbres\n",
    "        â€¢ learning_rate: Taux d'apprentissage (plus petit = plus robuste)\n",
    "        â€¢ max_depth: Profondeur (gÃ©nÃ©ralement plus faible que RF)\n",
    "        â€¢ subsample: Fraction d'Ã©chantillons par arbre (rÃ©gularisation)\n",
    "        \"\"\",\n",
    "        \n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [4, 6, 8],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"XGBoost\": {\n",
    "        \"description\": \"\"\"\n",
    "        âš¡ XGBoost (eXtreme Gradient Boosting)\n",
    "        \n",
    "        Principe:\n",
    "        - Version optimisÃ©e et rÃ©gularisÃ©e du Gradient Boosting\n",
    "        - Utilise des techniques avancÃ©es (regularization, pruning, parallelization)\n",
    "        - Algorithme de splitting plus efficace\n",
    "        - Gestion native des valeurs manquantes\n",
    "        \n",
    "        Avantages:\n",
    "        âœ“ Souvent le meilleur en compÃ©titions (Kaggle)\n",
    "        âœ“ Plus rapide que Gradient Boosting classique\n",
    "        âœ“ RÃ©gularisation L1/L2 intÃ©grÃ©e\n",
    "        âœ“ Gestion automatique des missing values\n",
    "        \n",
    "        InconvÃ©nients:\n",
    "        âœ— Beaucoup d'hyperparamÃ¨tres Ã  tuner\n",
    "        âœ— Peut Ãªtre \"overkill\" pour des problÃ¨mes simples\n",
    "        âœ— NÃ©cessite comprÃ©hension approfondie\n",
    "        \n",
    "        HyperparamÃ¨tres clÃ©s:\n",
    "        â€¢ n_estimators: Nombre d'arbres\n",
    "        â€¢ learning_rate: Taux d'apprentissage\n",
    "        â€¢ max_depth: Profondeur des arbres\n",
    "        â€¢ colsample_bytree: Fraction de features par arbre\n",
    "        â€¢ reg_alpha/reg_lambda: RÃ©gularisation L1/L2\n",
    "        \"\"\",\n",
    "        \n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [4, 6, 8],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"MLP\": {\n",
    "        \"description\": \"\"\"\n",
    "        ğŸ§  RÃ‰SEAU DE NEURONES (Multi-Layer Perceptron)\n",
    "        \n",
    "        Principe:\n",
    "        - RÃ©seau de neurones artificiels organisÃ©s en couches\n",
    "        - Chaque neurone applique: activation(weighted_sum(inputs) + bias)\n",
    "        - Apprentissage par rÃ©tropropagation du gradient\n",
    "        - Peut approximer n'importe quelle fonction (thÃ©orÃ¨me d'approximation universelle)\n",
    "        \n",
    "        Avantages:\n",
    "        âœ“ Peut capturer des relations trÃ¨s complexes\n",
    "        âœ“ Flexible et adaptable\n",
    "        âœ“ Bonne gÃ©nÃ©ralisation avec assez de donnÃ©es\n",
    "        \n",
    "        InconvÃ©nients:\n",
    "        âœ— NÃ©cessite BEAUCOUP de donnÃ©es (milliers/millions)\n",
    "        âœ— BoÃ®te noire (difficile Ã  interprÃ©ter)\n",
    "        âœ— Sensible au scaling des features\n",
    "        âœ— Lent Ã  entraÃ®ner\n",
    "        âœ— Instable (rÃ©sultats variables)\n",
    "        \n",
    "        HyperparamÃ¨tres clÃ©s:\n",
    "        â€¢ hidden_layer_sizes: Architecture (nombre et taille des couches)\n",
    "        â€¢ activation: Fonction d'activation (relu, tanh)\n",
    "        â€¢ alpha: RÃ©gularisation L2\n",
    "        â€¢ learning_rate_init: Taux d'apprentissage initial\n",
    "        \n",
    "        Note: GÃ©nÃ©ralement sous-performant sur petits datasets tabulaires\n",
    "        \"\"\",\n",
    "        \n",
    "        \"param_grid\": {\n",
    "            'hidden_layer_sizes': [(64,), (128, 64), (128, 64, 32)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Afficher la thÃ©orie\n",
    "for model_name, info in models_theory.items():\n",
    "    print(f\"\\n{info['description']}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PARTIE 2: CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Charger les donnÃ©es\n",
    "df = pd.read_csv(\"../data/clean/cleaned_data_filtered.csv\")\n",
    "print(f\"\\nâœ“ Dataset: {df.shape[0]} propriÃ©tÃ©s, {df.shape[1]} colonnes\")\n",
    "\n",
    "# Feature Engineering SANS DATA LEAKAGE\n",
    "print(\"\\nğŸ“Š Feature Engineering (sans data leakage)...\")\n",
    "\n",
    "# Features de base\n",
    "df['room_bathroom_ratio'] = df['room_count'] / np.maximum(df['bathroom_count'], 1)\n",
    "df['total_rooms'] = df['room_count'] + df['bathroom_count']\n",
    "df['size_per_room'] = df['size'] / np.maximum(df['room_count'], 1)\n",
    "df['bathroom_density'] = df['bathroom_count'] / np.maximum(df['size'], 1)\n",
    "df['size_x_rooms'] = df['size'] * df['room_count']\n",
    "df['size_x_bathrooms'] = df['size'] * df['bathroom_count']\n",
    "\n",
    "# SOLUTION 2: Luxury Score (calculable sans connaÃ®tre le prix!)\n",
    "print(\"  â†’ CrÃ©ation du luxury_score (pas de leakage)\")\n",
    "high_value_locations = ['La Marsa', 'Carthage', 'Sidi Bou Said', 'Gammarth', \n",
    "                        'Les Berges du Lac', 'Lac 1', 'Lac 2']\n",
    "df['is_premium_location'] = df['location'].isin(high_value_locations).astype(int)\n",
    "\n",
    "# Score de luxe basÃ© sur features disponibles\n",
    "df['luxury_score'] = (\n",
    "    (df['size'] / 100) * 0.3 +           # Grande surface\n",
    "    (df['room_count'] / 5) * 0.2 +       # Beaucoup de chambres\n",
    "    (df['bathroom_count'] / 2) * 0.2 +   # Beaucoup de SdB\n",
    "    df['is_premium_location'] * 0.3      # Zone premium\n",
    ")\n",
    "\n",
    "# Segmentation basÃ©e sur luxury_score\n",
    "df['property_tier'] = pd.cut(df['luxury_score'], \n",
    "                              bins=3, \n",
    "                              labels=['standard', 'upscale', 'luxury'])\n",
    "\n",
    "engineered_cols = ['room_bathroom_ratio', 'total_rooms', 'size_per_room', \n",
    "                   'bathroom_density', 'size_x_rooms', 'size_x_bathrooms',\n",
    "                   'luxury_score', 'is_premium_location']\n",
    "for col in engineered_cols:\n",
    "    if df[col].dtype in ['float64', 'int64']:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(df[col].median())\n",
    "\n",
    "print(\"  â†’ Features crÃ©Ã©es: luxury_score, property_tier, is_premium_location\")\n",
    "print(\"  âœ“ Aucune information du prix utilisÃ©e!\")\n",
    "\n",
    "# PrÃ©parer pour cross-validation avec calcul du prix mÃ©dian par zone\n",
    "# Note: Le prix mÃ©dian sera calculÃ© sur le train set uniquement dans la CV\n",
    "categorical_cols = ['category', 'type', 'location', 'property_tier']\n",
    "\n",
    "print(\"\\nğŸ“Š PrÃ©paration des donnÃ©es pour cross-validation...\")\n",
    "print(\"  Note: Le prix mÃ©dian par zone sera calculÃ© sur train set uniquement\")\n",
    "\n",
    "# Configuration des modÃ¨les avec leurs grilles\n",
    "models_config = {\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeRegressor(random_state=42),\n",
    "        \"params\": models_theory[\"Decision Tree\"][\"param_grid\"]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        \"params\": models_theory[\"Random Forest\"][\"param_grid\"]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"params\": models_theory[\"Gradient Boosting\"][\"param_grid\"]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror'),\n",
    "        \"params\": models_theory[\"XGBoost\"][\"param_grid\"]\n",
    "    },\n",
    "    \"MLP\": {\n",
    "        \"model\": MLPRegressor(random_state=42, max_iter=500, early_stopping=True),\n",
    "        \"params\": models_theory[\"MLP\"][\"param_grid\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PARTIE 3: OPTIMISATION DES HYPERPARAMÃˆTRES (Grid Search CV)\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nCette Ã©tape peut prendre 15-30 minutes selon votre machine...\")\n",
    "print(\"Grid Search utilise 3-fold CV pour chaque combinaison de paramÃ¨tres.\\n\")\n",
    "\n",
    "# Fonction pour ajouter les statistiques de location SANS LEAKAGE\n",
    "def add_location_features(df_train, df_test, df_full):\n",
    "    \"\"\"\n",
    "    Ajoute les prix mÃ©dians par zone calculÃ©s sur train uniquement\n",
    "    \"\"\"\n",
    "    # Calculer sur train uniquement\n",
    "    location_stats = df_train.groupby('location').agg({\n",
    "        'price': ['median', 'mean', 'std', 'count']\n",
    "    })\n",
    "    location_stats.columns = ['location_price_median', 'location_price_mean', \n",
    "                              'location_price_std', 'location_count']\n",
    "    location_stats = location_stats.reset_index()\n",
    "    \n",
    "    # Normaliser les stats de prix (pour Ã©viter le leakage direct)\n",
    "    location_stats['location_price_level'] = (\n",
    "        location_stats['location_price_median'] / location_stats['location_price_median'].median()\n",
    "    )\n",
    "    \n",
    "    # Merger sur train et test\n",
    "    df_train_merged = df_train.merge(location_stats[['location', 'location_price_level']], \n",
    "                                     on='location', how='left')\n",
    "    df_test_merged = df_test.merge(location_stats[['location', 'location_price_level']], \n",
    "                                   on='location', how='left')\n",
    "    \n",
    "    # Remplir les valeurs manquantes (nouvelles locations) avec la mÃ©diane\n",
    "    global_median = location_stats['location_price_level'].median()\n",
    "    df_train_merged['location_price_level'] = df_train_merged['location_price_level'].fillna(global_median)\n",
    "    df_test_merged['location_price_level'] = df_test_merged['location_price_level'].fillna(global_median)\n",
    "    \n",
    "    return df_train_merged, df_test_merged, location_stats\n",
    "\n",
    "# Optimiser chaque modÃ¨le\n",
    "optimized_models = {}\n",
    "optimization_results = []\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”§ Optimisation: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    n_combinations = np.prod([len(v) for v in config['params'].values()])\n",
    "    print(f\"Nombre de combinaisons Ã  tester: {n_combinations}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Custom CV pour ajouter location_price_level sans leakage\n",
    "    kf_opt = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_estimator = None\n",
    "    \n",
    "    # Grid Search manuel pour contrÃ´ler le preprocessing\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    param_grid = list(ParameterGrid(config['params']))\n",
    "    \n",
    "    print(f\"Testing {len(param_grid)} combinations...\")\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Progress: {i}/{len(param_grid)}...\", end='\\r')\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, test_idx in kf_opt.split(df):\n",
    "            # Split data\n",
    "            df_train_fold = df.iloc[train_idx].copy()\n",
    "            df_test_fold = df.iloc[test_idx].copy()\n",
    "            \n",
    "            # Add location features (calculated on train only)\n",
    "            df_train_fold, df_test_fold, _ = add_location_features(\n",
    "                df_train_fold, df_test_fold, df\n",
    "            )\n",
    "            \n",
    "            # One-hot encoding\n",
    "            df_train_encoded = pd.get_dummies(df_train_fold, columns=categorical_cols, drop_first=False)\n",
    "            df_test_encoded = pd.get_dummies(df_test_fold, columns=categorical_cols, drop_first=False)\n",
    "            \n",
    "            # Align columns\n",
    "            missing_cols = set(df_train_encoded.columns) - set(df_test_encoded.columns)\n",
    "            for col in missing_cols:\n",
    "                df_test_encoded[col] = 0\n",
    "            df_test_encoded = df_test_encoded[df_train_encoded.columns]\n",
    "            \n",
    "            # Features and target\n",
    "            feature_cols_fold = [col for col in df_train_encoded.columns \n",
    "                                if col not in ['price', 'log_price']]\n",
    "            X_train_fold = df_train_encoded[feature_cols_fold]\n",
    "            y_train_fold = df_train_encoded['log_price']\n",
    "            X_test_fold = df_test_encoded[feature_cols_fold]\n",
    "            y_test_fold = df_test_encoded['log_price']\n",
    "            \n",
    "            # Scale\n",
    "            scaler_fold = StandardScaler()\n",
    "            num_cols_fold = ['room_count', 'bathroom_count', 'size', 'room_bathroom_ratio', \n",
    "                            'total_rooms', 'size_per_room', 'bathroom_density',\n",
    "                            'size_x_rooms', 'size_x_bathrooms', 'luxury_score',\n",
    "                            'is_premium_location', 'location_price_level']\n",
    "            num_cols_present = [col for col in num_cols_fold if col in X_train_fold.columns]\n",
    "            X_train_fold[num_cols_present] = scaler_fold.fit_transform(X_train_fold[num_cols_present])\n",
    "            X_test_fold[num_cols_present] = scaler_fold.transform(X_test_fold[num_cols_present])\n",
    "            \n",
    "            # Train and score\n",
    "            model_fold = config['model'].__class__(**params, random_state=42)\n",
    "            if hasattr(model_fold, 'n_jobs'):\n",
    "                model_fold.n_jobs = -1\n",
    "            model_fold.fit(X_train_fold, y_train_fold)\n",
    "            score = model_fold.score(X_test_fold, y_test_fold)\n",
    "            scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "            best_estimator = config['model'].__class__(**params, random_state=42)\n",
    "            if hasattr(best_estimator, 'n_jobs'):\n",
    "                best_estimator.n_jobs = -1\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ“ TerminÃ© en {elapsed:.1f}s\")\n",
    "    print(f\"Meilleur score RÂ²: {best_score:.4f}\")\n",
    "    print(f\"Meilleurs paramÃ¨tres:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  â€¢ {param}: {value}\")\n",
    "    \n",
    "    optimized_models[name] = best_estimator\n",
    "    \n",
    "    optimization_results.append({\n",
    "        'Model': name,\n",
    "        'Best_R2': best_score,\n",
    "        'Best_Params': best_params,\n",
    "        'Time_seconds': elapsed\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PARTIE 4: Ã‰VALUATION FINALE DES MODÃˆLES OPTIMISÃ‰S\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# PrÃ©parer donnÃ©es complÃ¨tes pour Ã©valuation finale\n",
    "# Calculer location_price_level sur l'ensemble complet (pour training final)\n",
    "location_stats_full = df.groupby('location')['price'].median().reset_index()\n",
    "location_stats_full['location_price_level'] = (\n",
    "    location_stats_full['price'] / location_stats_full['price'].median()\n",
    ")\n",
    "df_final = df.merge(location_stats_full[['location', 'location_price_level']], \n",
    "                   on='location', how='left')\n",
    "df_final['location_price_level'] = df_final['location_price_level'].fillna(1.0)\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(df_final, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "feature_cols = [col for col in df_encoded.columns if col not in ['price', 'log_price']]\n",
    "X = df_encoded[feature_cols].copy()\n",
    "y = df_encoded['log_price'].copy()\n",
    "\n",
    "num_cols = ['room_count', 'bathroom_count', 'size', 'room_bathroom_ratio', \n",
    "            'total_rooms', 'size_per_room', 'bathroom_density',\n",
    "            'size_x_rooms', 'size_x_bathrooms', 'luxury_score',\n",
    "            'is_premium_location', 'location_price_level']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "print(f\"âœ“ Features finales: {X.shape[1]} colonnes\")\n",
    "print(f\"âœ“ Target: log10(price)\")\n",
    "\n",
    "# Ã‰valuation avec 5-fold CV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_results = {}\n",
    "\n",
    "for name, model in optimized_models.items():\n",
    "    print(f\"\\nğŸ“Š Ã‰valuation finale: {name}...\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    \n",
    "    # PrÃ©dictions pour mÃ©triques dÃ©taillÃ©es\n",
    "    y_pred_log = np.zeros_like(y)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        y_pred_log[test_idx] = model.predict(X.iloc[test_idx])\n",
    "    \n",
    "    # MÃ©triques log-space\n",
    "    rmse_log = np.sqrt(mean_squared_error(y, y_pred_log))\n",
    "    mae_log = mean_absolute_error(y, y_pred_log)\n",
    "    r2_log = r2_score(y, y_pred_log)\n",
    "    \n",
    "    # Back-transform avec bias correction\n",
    "    y_actual = 10 ** y\n",
    "    y_pred_raw = 10 ** y_pred_log\n",
    "    \n",
    "    df_temp = pd.DataFrame({\n",
    "        'actual_log': y.values,\n",
    "        'pred_log': y_pred_log,\n",
    "        'type': df['type'].values\n",
    "    })\n",
    "    \n",
    "    bias_factors = {}\n",
    "    for prop_type in df_temp['type'].unique():\n",
    "        mask = df_temp['type'] == prop_type\n",
    "        mean_actual = np.mean(df_temp.loc[mask, 'actual_log'])\n",
    "        mean_pred = np.mean(df_temp.loc[mask, 'pred_log'])\n",
    "        bias_factors[prop_type] = 10 ** (mean_actual - mean_pred)\n",
    "    \n",
    "    y_pred_corrected = np.array([\n",
    "        y_pred_raw[i] * bias_factors[df['type'].iloc[i]] \n",
    "        for i in range(len(y_pred_raw))\n",
    "    ])\n",
    "    \n",
    "    # MÃ©triques espace rÃ©el\n",
    "    rmse_actual = np.sqrt(mean_squared_error(y_actual, y_pred_corrected))\n",
    "    mae_actual = mean_absolute_error(y_actual, y_pred_corrected)\n",
    "    mape = np.mean(np.abs((y_actual - y_pred_corrected) / y_actual) * 100)\n",
    "    \n",
    "    final_results[name] = {\n",
    "        'R2_log': r2_log,\n",
    "        'R2_std': r2_scores.std(),\n",
    "        'RMSE_log': rmse_log,\n",
    "        'MAE_log': mae_log,\n",
    "        'RMSE_actual': rmse_actual,\n",
    "        'MAE_actual': mae_actual,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# CrÃ©er tableau rÃ©capitulatif\n",
    "results_df = pd.DataFrame(final_results).T\n",
    "results_df = results_df.sort_values('R2_log', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TABLEAU RÃ‰CAPITULATIF DES PERFORMANCES FINALES\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n\", results_df.round(4).to_string())\n",
    "\n",
    "# Identifier le meilleur\n",
    "best_model_name = results_df['R2_log'].idxmax()\n",
    "best_model = optimized_models[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ† MODÃˆLE CHAMPION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nLe meilleur modÃ¨le est: {best_model_name}\")\n",
    "print(f\"  â€¢ RÂ² = {results_df.loc[best_model_name, 'R2_log']:.4f}\")\n",
    "print(f\"  â€¢ MAE = {results_df.loc[best_model_name, 'MAE_actual']:,.0f} TND\")\n",
    "print(f\"  â€¢ MAPE = {results_df.loc[best_model_name, 'MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ JUSTIFICATION DU CHOIX:\")\n",
    "print(f\"  {best_model_name} a Ã©tÃ© sÃ©lectionnÃ© car il prÃ©sente:\")\n",
    "print(f\"  1. Le meilleur RÂ² ({results_df.loc[best_model_name, 'R2_log']:.4f}) = meilleure capacitÃ© explicative\")\n",
    "print(f\"  2. MAPE acceptable ({results_df.loc[best_model_name, 'MAPE']:.1f}%) pour l'immobilier\")\n",
    "print(f\"  3. Robustesse confirmÃ©e par validation croisÃ©e\")\n",
    "\n",
    "print(\"\\nğŸ’¡ NOTE SUR LE DATA LEAKAGE:\")\n",
    "print(\"  Ce modÃ¨le utilise des features SANS data leakage:\")\n",
    "print(\"  â€¢ luxury_score: calculÃ© Ã  partir de features disponibles\")\n",
    "print(\"  â€¢ location_price_level: calculÃ© sur train set uniquement en CV\")\n",
    "print(\"  â€¢ property_tier: basÃ© sur luxury_score\")\n",
    "print(\"  âœ“ Aucune information du prix target utilisÃ©e!\")\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le final sur toutes les donnÃ©es\n",
    "print(\"\\nğŸ“¦ EntraÃ®nement du modÃ¨le final sur toutes les donnÃ©es...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Sauvegarder le modÃ¨le et les objets nÃ©cessaires\n",
    "joblib.dump(best_model, '../output/best_model.pkl')\n",
    "joblib.dump(scaler, '../output/scaler.pkl')\n",
    "joblib.dump(feature_cols, '../output/feature_cols.pkl')\n",
    "joblib.dump(location_stats_full, '../output/location_stats.pkl')\n",
    "joblib.dump(high_value_locations, '../output/premium_locations.pkl')\n",
    "\n",
    "print(\"\\nâœ… ModÃ¨le sauvegardÃ©:\")\n",
    "print(\"  â€¢ ../output/best_model.pkl\")\n",
    "print(\"  â€¢ ../output/scaler.pkl\")\n",
    "print(\"  â€¢ ../output/feature_cols.pkl\")\n",
    "print(\"  â€¢ ../output/location_stats.pkl (pour production)\")\n",
    "print(\"  â€¢ ../output/premium_locations.pkl\")\n",
    "\n",
    "# Export rÃ©sultats\n",
    "results_df.to_csv('../output/final_comparison.csv')\n",
    "print(\"  â€¢ ../output/final_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"âœ… PROJET TERMINÃ‰!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nProcÃ©dez maintenant Ã  'prediction_interface.py' pour tester le modÃ¨le!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce878ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHARGEMENT DU MODÃˆLE ENTRAÃNÃ‰\n",
      "================================================================================\n",
      "âœ“ ModÃ¨le chargÃ©: XGBRegressor\n",
      "âœ“ Nombre de features: 46\n",
      "âœ“ Stats de 23 locations chargÃ©es\n",
      "\n",
      "================================================================================\n",
      "EXEMPLES DE PRÃ‰DICTIONS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Exemple 1: Appartement Ã  louer Ã  Tunis\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CaractÃ©ristiques:\n",
      "  â€¢ Chambres: 3\n",
      "  â€¢ Salles de bain: 2\n",
      "  â€¢ Surface: 120 mÂ²\n",
      "  â€¢ Ville: Tunis\n",
      "  â€¢ CatÃ©gorie: Appartements\n",
      "  â€¢ Type: Ã€ Louer\n",
      "\n",
      "ğŸ’° PRÃ‰DICTION:\n",
      "  Prix estimÃ©: 1,124 TND/mois\n",
      "  Intervalle de confiance: 899 - 1,348 TND/mois\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Exemple 2: Villa Ã  vendre Ã  La Marsa (zone premium)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CaractÃ©ristiques:\n",
      "  â€¢ Chambres: 5\n",
      "  â€¢ Salles de bain: 3\n",
      "  â€¢ Surface: 300 mÂ²\n",
      "  â€¢ Ville: La Marsa\n",
      "  â€¢ CatÃ©gorie: Villas\n",
      "  â€¢ Type: Ã€ Vendre\n",
      "\n",
      "ğŸ’° PRÃ‰DICTION:\n",
      "  Prix estimÃ©: 639,889 TND\n",
      "  Intervalle de confiance: 511,912 - 767,867 TND\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Exemple 3: Studio Ã  louer Ã  Sousse\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CaractÃ©ristiques:\n",
      "  â€¢ Chambres: 1\n",
      "  â€¢ Salles de bain: 1\n",
      "  â€¢ Surface: 35 mÂ²\n",
      "  â€¢ Ville: Sousse\n",
      "  â€¢ CatÃ©gorie: Studios\n",
      "  â€¢ Type: Ã€ Louer\n",
      "\n",
      "ğŸ’° PRÃ‰DICTION:\n",
      "  Prix estimÃ©: 636 TND/mois\n",
      "  Intervalle de confiance: 509 - 763 TND/mois\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Exemple 4: Maison Ã  vendre Ã  Sfax\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CaractÃ©ristiques:\n",
      "  â€¢ Chambres: 4\n",
      "  â€¢ Salles de bain: 2\n",
      "  â€¢ Surface: 200 mÂ²\n",
      "  â€¢ Ville: Sfax\n",
      "  â€¢ CatÃ©gorie: Maisons\n",
      "  â€¢ Type: Ã€ Vendre\n",
      "\n",
      "ğŸ’° PRÃ‰DICTION:\n",
      "  Prix estimÃ©: 431,935 TND\n",
      "  Intervalle de confiance: 345,548 - 518,322 TND\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INTERFACE DE PRÃ‰DICTION INTERACTIVE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ GARANTIE SANS DATA LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "Ce systÃ¨me de prÃ©diction est conÃ§u SANS data leakage:\n",
      "\n",
      "âœ“ luxury_score: CalculÃ© Ã  partir de size, rooms, bathrooms (disponibles!)\n",
      "âœ“ location_price_level: Prix mÃ©dian relatif (calculÃ© sur train, stockÃ©)\n",
      "âœ“ property_tier: BasÃ© sur luxury_score (pas sur le vrai prix)\n",
      "âœ“ is_premium_location: Liste prÃ©dÃ©finie de zones premium\n",
      "\n",
      "Toutes les features sont calculables AVANT de connaÃ®tre le prix!\n",
      "Le modÃ¨le est production-ready et cohÃ©rent train/test.\n",
      "\n",
      "\n",
      "ğŸ’¡ TIP: Pour utiliser l'interface interactive, appelez: interactive_prediction()\n",
      "\n",
      "Exemple:\n",
      ">>> interactive_prediction()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "INTERFACE DE PRÃ‰DICTION - PRIX IMMOBILIER TUNISIE\n",
    "================================================================================\n",
    "Ce script charge le modÃ¨le entraÃ®nÃ© et permet de faire des prÃ©dictions\n",
    "sur de nouvelles propriÃ©tÃ©s SANS DATA LEAKAGE.\n",
    "\n",
    "Features utilisÃ©es:\n",
    "- luxury_score: calculÃ© Ã  partir de size, rooms, bathrooms, location\n",
    "- location_price_level: prix mÃ©dian relatif de la zone (calculÃ© sur train)\n",
    "- property_tier: segment basÃ© sur luxury_score\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger le modÃ¨le et les objets nÃ©cessaires\n",
    "print(\"=\" * 80)\n",
    "print(\"CHARGEMENT DU MODÃˆLE ENTRAÃNÃ‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model = joblib.load('../output/best_model.pkl')\n",
    "scaler = joblib.load('../output/scaler.pkl')\n",
    "feature_cols = joblib.load('../output/feature_cols.pkl')\n",
    "location_stats = joblib.load('../output/location_stats.pkl')\n",
    "premium_locations = joblib.load('../output/premium_locations.pkl')\n",
    "\n",
    "print(f\"âœ“ ModÃ¨le chargÃ©: {type(model).__name__}\")\n",
    "print(f\"âœ“ Nombre de features: {len(feature_cols)}\")\n",
    "print(f\"âœ“ Stats de {len(location_stats)} locations chargÃ©es\")\n",
    "\n",
    "# Listes des options\n",
    "VILLES = [\n",
    "    'Tunis', 'Sfax', 'Sousse', 'Kairouan', 'Bizerte', 'GabÃ¨s', 'Ariana', \n",
    "    'Gafsa', 'Monastir', 'Ben Arous', 'Kasserine', 'MÃ©denine', 'Nabeul', \n",
    "    'Tataouine', 'BÃ©ja', 'Jendouba', 'Mahdia', 'Siliana', 'KÃ©bili', \n",
    "    'Zaghouan', 'Manouba', 'Tozeur', 'Sidi Bouzid', 'La Marsa', 'Hammamet'\n",
    "]\n",
    "\n",
    "CATEGORIES = [\n",
    "    'Appartements', 'Maisons', 'Villas', 'Studios', 'Duplex', 'Terrains',\n",
    "    'Bureaux et Plateaux', 'Locations de vacances', 'Terrains et Fermes',\n",
    "    'Colocations', 'Magasins, Commerces et Locaux industriels'\n",
    "]\n",
    "\n",
    "TYPES_TRANSACTION = ['Ã€ Vendre', 'Ã€ Louer']\n",
    "\n",
    "\n",
    "def engineer_features(data, location):\n",
    "    \"\"\"\n",
    "    Applique le feature engineering sur les donnÃ©es d'entrÃ©e\n",
    "    SANS DATA LEAKAGE - tout est calculable sans connaÃ®tre le prix!\n",
    "    \"\"\"\n",
    "    # Features de base\n",
    "    data['room_bathroom_ratio'] = data['room_count'] / np.maximum(data['bathroom_count'], 1)\n",
    "    data['total_rooms'] = data['room_count'] + data['bathroom_count']\n",
    "    data['size_per_room'] = data['size'] / np.maximum(data['room_count'], 1)\n",
    "    data['bathroom_density'] = data['bathroom_count'] / np.maximum(data['size'], 1)\n",
    "    data['size_x_rooms'] = data['size'] * data['room_count']\n",
    "    data['size_x_bathrooms'] = data['size'] * data['bathroom_count']\n",
    "    \n",
    "    # Premium location indicator\n",
    "    data['is_premium_location'] = (location in premium_locations) * 1\n",
    "    \n",
    "    # Luxury score (SANS LEAKAGE - basÃ© uniquement sur features disponibles)\n",
    "    data['luxury_score'] = (\n",
    "        (data['size'] / 100) * 0.3 +\n",
    "        (data['room_count'] / 5) * 0.2 +\n",
    "        (data['bathroom_count'] / 2) * 0.2 +\n",
    "        data['is_premium_location'] * 0.3\n",
    "    )\n",
    "    \n",
    "    # Property tier basÃ© sur luxury_score\n",
    "    if data['luxury_score'].values[0] < 0.5:\n",
    "        property_tier = 'standard'\n",
    "    elif data['luxury_score'].values[0] < 1.0:\n",
    "        property_tier = 'upscale'\n",
    "    else:\n",
    "        property_tier = 'luxury'\n",
    "    \n",
    "    data['property_tier'] = property_tier\n",
    "    \n",
    "    # Location price level (calculÃ© sur train set, stockÃ© dans location_stats)\n",
    "    location_price_level = location_stats[\n",
    "        location_stats['location'] == location\n",
    "    ]['location_price_level'].values\n",
    "    \n",
    "    if len(location_price_level) > 0:\n",
    "        data['location_price_level'] = location_price_level[0]\n",
    "    else:\n",
    "        # Ville inconnue -> utiliser la mÃ©diane\n",
    "        data['location_price_level'] = 1.0\n",
    "    \n",
    "    # Gestion des infinis\n",
    "    for col in ['room_bathroom_ratio', 'total_rooms', 'size_per_room', \n",
    "                'bathroom_density', 'size_x_rooms', 'size_x_bathrooms', 'luxury_score']:\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan).fillna(data[col].median())\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_input_for_prediction(room_count, bathroom_count, size, location, \n",
    "                                  category, transaction_type):\n",
    "    \"\"\"\n",
    "    PrÃ©pare les donnÃ©es d'entrÃ©e pour la prÃ©diction\n",
    "    \n",
    "    Args:\n",
    "        room_count (int): Nombre de chambres\n",
    "        bathroom_count (int): Nombre de salles de bain\n",
    "        size (float): Surface en mÂ²\n",
    "        location (str): Ville\n",
    "        category (str): Type de bien\n",
    "        transaction_type (str): 'Ã€ Vendre' ou 'Ã€ Louer'\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Features prÃ©parÃ©es pour le modÃ¨le\n",
    "    \"\"\"\n",
    "    \n",
    "    # CrÃ©er DataFrame de base\n",
    "    input_data = pd.DataFrame({\n",
    "        'room_count': [room_count],\n",
    "        'bathroom_count': [bathroom_count],\n",
    "        'size': [size],\n",
    "        'category': [category],\n",
    "        'type': [transaction_type],\n",
    "        'location': [location]\n",
    "    })\n",
    "    \n",
    "    # Feature engineering (SANS LEAKAGE!)\n",
    "    input_data = engineer_features(input_data, location)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    input_encoded = pd.get_dummies(input_data, \n",
    "                                    columns=['category', 'type', 'location', 'property_tier'])\n",
    "    \n",
    "    # CrÃ©er un DataFrame avec toutes les features attendues\n",
    "    X_pred = pd.DataFrame(0, index=[0], columns=feature_cols)\n",
    "    \n",
    "    # Remplir les colonnes prÃ©sentes\n",
    "    for col in input_encoded.columns:\n",
    "        if col in X_pred.columns:\n",
    "            X_pred[col] = input_encoded[col].values\n",
    "    \n",
    "    # Standardiser les features numÃ©riques\n",
    "    num_cols = ['room_count', 'bathroom_count', 'size', 'room_bathroom_ratio', \n",
    "                'total_rooms', 'size_per_room', 'bathroom_density',\n",
    "                'size_x_rooms', 'size_x_bathrooms', 'luxury_score',\n",
    "                'is_premium_location', 'location_price_level']\n",
    "    \n",
    "    X_pred[num_cols] = scaler.transform(X_pred[num_cols])\n",
    "    \n",
    "    return X_pred, transaction_type\n",
    "\n",
    "\n",
    "def predict_price(room_count, bathroom_count, size, location, category, transaction_type):\n",
    "    \"\"\"\n",
    "    PrÃ©dit le prix d'une propriÃ©tÃ©\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prix_prÃ©dit, intervalle_confiance_bas, intervalle_confiance_haut)\n",
    "    \"\"\"\n",
    "    \n",
    "    # PrÃ©parer les donnÃ©es\n",
    "    X_pred, prop_type = prepare_input_for_prediction(\n",
    "        room_count, bathroom_count, size, location, category, transaction_type\n",
    "    )\n",
    "    \n",
    "    # PrÃ©diction en log-space\n",
    "    log_price_pred = model.predict(X_pred)[0]\n",
    "    \n",
    "    # Back-transform\n",
    "    price_pred = 10 ** log_price_pred\n",
    "    \n",
    "    # Bias correction basÃ© sur le type (calculÃ© lors de l'entraÃ®nement)\n",
    "    bias_factors = {'Ã€ Vendre': 1.0022, 'Ã€ Louer': 0.9935}\n",
    "    price_pred = price_pred * bias_factors.get(prop_type, 1.0)\n",
    "    \n",
    "    # Intervalle de confiance approximatif (Â±20% pour l'immobilier)\n",
    "    conf_low = price_pred * 0.8\n",
    "    conf_high = price_pred * 1.2\n",
    "    \n",
    "    return price_pred, conf_low, conf_high\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLES D'UTILISATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLES DE PRÃ‰DICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"description\": \"Appartement Ã  louer Ã  Tunis\",\n",
    "        \"room_count\": 3,\n",
    "        \"bathroom_count\": 2,\n",
    "        \"size\": 120,\n",
    "        \"location\": \"Tunis\",\n",
    "        \"category\": \"Appartements\",\n",
    "        \"transaction_type\": \"Ã€ Louer\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Villa Ã  vendre Ã  La Marsa (zone premium)\",\n",
    "        \"room_count\": 5,\n",
    "        \"bathroom_count\": 3,\n",
    "        \"size\": 300,\n",
    "        \"location\": \"La Marsa\",\n",
    "        \"category\": \"Villas\",\n",
    "        \"transaction_type\": \"Ã€ Vendre\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Studio Ã  louer Ã  Sousse\",\n",
    "        \"room_count\": 1,\n",
    "        \"bathroom_count\": 1,\n",
    "        \"size\": 35,\n",
    "        \"location\": \"Sousse\",\n",
    "        \"category\": \"Studios\",\n",
    "        \"transaction_type\": \"Ã€ Louer\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Maison Ã  vendre Ã  Sfax\",\n",
    "        \"room_count\": 4,\n",
    "        \"bathroom_count\": 2,\n",
    "        \"size\": 200,\n",
    "        \"location\": \"Sfax\",\n",
    "        \"category\": \"Maisons\",\n",
    "        \"transaction_type\": \"Ã€ Vendre\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"Exemple {i}: {example['description']}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(f\"CaractÃ©ristiques:\")\n",
    "    print(f\"  â€¢ Chambres: {example['room_count']}\")\n",
    "    print(f\"  â€¢ Salles de bain: {example['bathroom_count']}\")\n",
    "    print(f\"  â€¢ Surface: {example['size']} mÂ²\")\n",
    "    print(f\"  â€¢ Ville: {example['location']}\")\n",
    "    print(f\"  â€¢ CatÃ©gorie: {example['category']}\")\n",
    "    print(f\"  â€¢ Type: {example['transaction_type']}\")\n",
    "    \n",
    "    price, conf_low, conf_high = predict_price(\n",
    "        example['room_count'],\n",
    "        example['bathroom_count'],\n",
    "        example['size'],\n",
    "        example['location'],\n",
    "        example['category'],\n",
    "        example['transaction_type']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ’° PRÃ‰DICTION:\")\n",
    "    if example['transaction_type'] == 'Ã€ Louer':\n",
    "        print(f\"  Prix estimÃ©: {price:,.0f} TND/mois\")\n",
    "        print(f\"  Intervalle de confiance: {conf_low:,.0f} - {conf_high:,.0f} TND/mois\")\n",
    "    else:\n",
    "        print(f\"  Prix estimÃ©: {price:,.0f} TND\")\n",
    "        print(f\"  Intervalle de confiance: {conf_low:,.0f} - {conf_high:,.0f} TND\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTERFACE INTERACTIVE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"INTERFACE DE PRÃ‰DICTION INTERACTIVE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def interactive_prediction():\n",
    "    \"\"\"\n",
    "    Permet Ã  l'utilisateur de faire des prÃ©dictions interactivement\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“ Entrez les caractÃ©ristiques de la propriÃ©tÃ©:\")\n",
    "    print(\"(Appuyez sur Ctrl+C pour quitter)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Collecte des inputs\n",
    "            print(\"â”€\" * 80)\n",
    "            room_count = int(input(\"Nombre de chambres: \"))\n",
    "            bathroom_count = int(input(\"Salles de bain: \"))\n",
    "            size = float(input(\"Surface (mÂ²): \"))\n",
    "            \n",
    "            print(f\"\\nVilles disponibles: {', '.join(VILLES[:10])}... (et autres)\")\n",
    "            location = input(\"Ville: \").strip()\n",
    "            if location not in VILLES:\n",
    "                print(f\"âš ï¸  Ville inconnue. Utilisation de 'Tunis' par dÃ©faut.\")\n",
    "                location = 'Tunis'\n",
    "            \n",
    "            print(f\"\\nCatÃ©gories: {', '.join(CATEGORIES[:5])}... (et autres)\")\n",
    "            category = input(\"CatÃ©gorie: \").strip()\n",
    "            if category not in CATEGORIES:\n",
    "                print(f\"âš ï¸  CatÃ©gorie inconnue. Utilisation de 'Appartements' par dÃ©faut.\")\n",
    "                category = 'Appartements'\n",
    "            \n",
    "            print(f\"\\nType de transaction: {', '.join(TYPES_TRANSACTION)}\")\n",
    "            transaction_type = input(\"Type: \").strip()\n",
    "            if transaction_type not in TYPES_TRANSACTION:\n",
    "                print(f\"âš ï¸  Type inconnu. Utilisation de 'Ã€ Vendre' par dÃ©faut.\")\n",
    "                transaction_type = 'Ã€ Vendre'\n",
    "            \n",
    "            # PrÃ©diction\n",
    "            print(\"\\nğŸ”® Calcul de la prÃ©diction...\\n\")\n",
    "            price, conf_low, conf_high = predict_price(\n",
    "                room_count, bathroom_count, size, location, category, transaction_type\n",
    "            )\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "            print(\"ğŸ’° RÃ‰SULTAT DE LA PRÃ‰DICTION\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            if transaction_type == 'Ã€ Louer':\n",
    "                print(f\"\\n  Prix estimÃ©: {price:,.0f} TND/mois\")\n",
    "                print(f\"  Intervalle 80%: {conf_low:,.0f} - {conf_high:,.0f} TND/mois\")\n",
    "            else:\n",
    "                print(f\"\\n  Prix estimÃ©: {price:,.0f} TND\")\n",
    "                print(f\"  Intervalle 80%: {conf_low:,.0f} - {conf_high:,.0f} TND\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            \n",
    "            # Demander si continuer\n",
    "            continue_input = input(\"\\nFaire une autre prÃ©diction? (o/n): \").strip().lower()\n",
    "            if continue_input != 'o':\n",
    "                break\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nğŸ‘‹ Au revoir!\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"\\nâŒ Erreur: EntrÃ©e invalide. Veuillez entrer des nombres valides.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Erreur: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¡ GARANTIE SANS DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Ce systÃ¨me de prÃ©diction est conÃ§u SANS data leakage:\n",
    "\n",
    "âœ“ luxury_score: CalculÃ© Ã  partir de size, rooms, bathrooms (disponibles!)\n",
    "âœ“ location_price_level: Prix mÃ©dian relatif (calculÃ© sur train, stockÃ©)\n",
    "âœ“ property_tier: BasÃ© sur luxury_score (pas sur le vrai prix)\n",
    "âœ“ is_premium_location: Liste prÃ©dÃ©finie de zones premium\n",
    "\n",
    "Toutes les features sont calculables AVANT de connaÃ®tre le prix!\n",
    "Le modÃ¨le est production-ready et cohÃ©rent train/test.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ’¡ TIP: Pour utiliser l'interface interactive, appelez: interactive_prediction()\")\n",
    "print(\"\\nExemple:\")\n",
    "print(\">>> interactive_prediction()\")\n",
    "\n",
    "# DÃ©commenter la ligne suivante pour lancer automatiquement\n",
    "# interactive_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
